{
 "metadata": {
  "name": "",
  "signature": "sha256:ff464d32cfc1eee1a9ce49a98acf2e832211ea1e213f4133f51aa945c0b4e5a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Algorithmes de descente en optimisation diff\u00e9rentiable sans contrainte"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mettez ci-dessous les imports classiques de librairie Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dans ce TP, nous nous int\u00e9ressons aux algorithmes pour la minimisation sans contrainte de fonctionnelles tr\u00e8s g\u00e9n\u00e9rales: le probl\u00e8me s'\u00e9crit:\n",
      "\n",
      "$$\\min_{x\\in\\mathbb{R}^n} f(x).$$\n",
      "\n",
      "Le but de cette s\u00e9ance est l'\u00e9criture d'un code de minimisation locale, et l'\u00e9valuation de ses performances sur les fonctions tests suivantes:\n",
      "\n",
      "<li>$f_1(x,y) = 2(x+y-2)^2+(x-y)^2$.\n",
      "<li>$f_2(x,y) = 100(y-x^2)^2 + (1-x)^2$ (fonction de Rosenbrock).\n",
      "\n",
      "On appelle $\\textit{oracle}$ une routine qui \u00e0 un $x$ donn\u00e9, renvoie la valeur $f(x)$ du crit\u00e8re, le gradient $\\nabla f(x)$ (ou une approximation du gradient) s'il existe, et \u00e9ventuellement la matrice Hessienne $H[f](x)$ (ou une approximation) si elle existe et si n\u00e9cessaire:\n",
      "\n",
      "$$[f(x),\\nabla f(x),H[f](x)] = \\textrm{oracle}(x)$$\n",
      "\n",
      "> **A faire :** Calculer les gradients et les Hessiennes des deux fonctions propos\u00e9es et impl\u00e9menter les fonctions $\\textrm{oracle}$ correspondantes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">**R\u00e9ponse :** *On trouve les gradients\n",
      "$$\\nabla f_1(x) = \\begin{pmatrix}6x+2y-8\\\\6y+2x-8\\end{pmatrix} \\quad\\mbox{ et }\\quad \\nabla f_2(x) = \\begin{pmatrix}400(x^3-yx)+2(x-1)\\\\200(y-x^2)\\end{pmatrix}.$$\n",
      "Ainsi que les hessiennes\n",
      "$$H[f_1](x) = \\begin{pmatrix}6&2\\\\2&6\\end{pmatrix} \\quad\\mbox{ et }\\quad H[f_2](x) = \\begin{pmatrix}1200x^2 - 400y + 2 & -400x\\\\-400x&200\\end{pmatrix} .$$* "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def oracle1(x):\n",
      "    return f,df,Hf\n",
      "\n",
      "def oracle2(x):\n",
      "    return f,df,Hf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On rappelle qu'un algorithme de descente appliqu\u00e9 \u00e0 la minimisation de $f$ poss\u00e8de la forme suivante:\n",
      "\n",
      "* **Donn\u00e9es :** $x_0\\in \\mathbb{R}^n$ point initial arbitraire, un oracle.\n",
      "* **Initialisation :** Num\u00e9ro d'it\u00e9ration: $k=0$.\n",
      "* **Tant que** le crit\u00e8re d'arr\u00eat n'est pas satisfait, **faire**\n",
      "  * Calcul de la direction de descente $d_k$.\n",
      "  * Choix/Calcul du pas $s_k$.\n",
      "  * Mise \u00e0 jour: calcul du prochain it\u00e9r\u00e9 $x_{k+1}$.\n",
      "  * $k = k + 1.$\n",
      "\n",
      "> **A faire :** R\u00e9solution math\u00e9matique:\n",
      "\n",
      "1. Donner les points critiques des fonctions propos\u00e9es.\n",
      "2. Les fonctions $f_i$ admettent-elles des extrema sur $\\mathbb{R}^2$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">**R\u00e9ponse :** *Les points critiques $x^*_i\\in\\mathbb{R}^2$, $i\\in\\{1,2\\},$ des fonctions $f_i$ v\u00e9rifie\n",
      "$$\\nabla f_1(x^\\star_1) = 0 \\quad \\Leftrightarrow \\quad x^\\star_1 = \\begin{pmatrix} 1\\\\1 \\end{pmatrix},$$\n",
      "et\n",
      "$$\\nabla f_2(x^\\star_2) = 0 \\quad \\Leftrightarrow \\quad x^\\star_2 = \\begin{pmatrix} 1\\\\1 \\end{pmatrix}.$$\n",
      "On remarque que\n",
      "$$\\det(H[f_1](x^\\star_1)) = \\det\\begin{pmatrix}6&2\\\\2&6\\end{pmatrix}=32>0 \\quad\\mbox{ et }\\quad \\textrm{Tr}(H[f_1](x^\\star_1)) = 12>0,$$\n",
      "donc $x^\\star_1$ est un minimum localde la fonction $f_1$.\n",
      "De m\u00eame, sachant que\n",
      "$$\\det(H[f_2](x^\\star_2)) = \\begin{pmatrix}802 & -400\\\\-400&200\\end{pmatrix} = 400\\quad\\mbox{ et }\\quad \\textrm{Tr}(H[f_2](x^\\star_2)) >0,$$\n",
      "on d\u00e9duit que $x^\\star_2$ est un minimum local.\n",
      "Les minimums sont m\u00eame globaux car les fonctions $f_i$ sont $\\ge 0$ et sont nulles en $x_i^\\star$*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> **A faire :** Impl\u00e9menter deux algorithmes de mimimisation, un par m\u00e9thode de gradient \u00e0 pas fixe et l'autre par une m\u00e9thode de Newton locale. Les arguments d'entr\u00e9e sont $\\textrm{function}$ qui est l'oracle \u00e0 minimiser, $\\textrm{xini}$ qui est le point initial et $\\textrm{h}$ qui est le pas de la m\u00e9thode de gradient. Les arguments de sortie sont $\\textrm{x}$ la valeur finale du point trouv\u00e9e, $\\textrm{xiter}$ qui est la valeur du point au cours des it\u00e9rations et $\\textrm{iter}$ le nombre d'it\u00e9rations pour arriver \u00e0 convergence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Gradient(function,h=1e-1,xini=np.array([0,0])):\n",
      "    return x,xiter,iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Newton(function,xini=[0,0]):\n",
      "    return x,xiter,iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> **A faire :** En utilisant le module $\\textrm{matplotlib.pyplot}$, on veut repr\u00e9senter/dessiner la suite des it\u00e9r\u00e9s dans $\\mathbb{R}^2$ avec la fonction $\\textrm{scatter}$.On souhaite aussi repr\u00e9senter les fonctions $f_1$ et $f_2$ en utilisant la fonction $\\textrm{contour}$ du module $\\textrm{matplotlib.pyplot}$. On s'inspirera du code qui suit pour cr\u00e9er deux fonctions (une pour chaque oracle) $\\textrm{affichage1(xiter)}$ et $\\textrm{affichage2(xiter)}$ qui affiche repr\u00e9sente les it\u00e9rations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Nx = 1000\n",
      "Ny = 1000\n",
      "x = np.linspace(-2,2,Nx)\n",
      "y = np.linspace(-2,2,Ny)\n",
      "X, Y = np.meshgrid(x, y)\n",
      "Z = 2*(X+Y-2)**2+(X-Y)**2\n",
      "CS=plt.contour(X, Y, Z,[0,0.1,1,2,4,6,8,12,16,20,24],colors='k')\n",
      "plt.clabel(CS, inline=1, fontsize=10)\n",
      "z = [[1,1.5],[0,0.5]]\n",
      "plt.scatter(z[0],z[1],marker='o')\n",
      "plt.show()\n",
      "\n",
      "def affichage1(xiter) :\n",
      "    pass\n",
      "def affichage2(xiter) :\n",
      "    pass\n",
      "affichage1(np.array(z))\n",
      "plt.show()\n",
      "affichage2(np.array(z))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tester la m\u00e9thode de Newton et la m\u00e9thode de gradient pour diff\u00e9rents points de d\u00e9part et diff\u00e9rents pas pour la fonction oracle1. Conclure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}